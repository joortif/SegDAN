####################################
# DATASET ANALYSIS
####################################

# analyze: This parameter is used to perform an analysis of the image dataset and its labels.
# If the dataset_path only contains images then only their sizes are shown.
# If labels are also provided, then a deeper analysis will be performed: 
# - Labels will be converted to multiclass if necessary.
# - Image and label sizes will be analyzed, as well as their correspondency. They must have the same file names.
# - The number of classes in the whole dataset will be calculated from the labels.
# - Metrics are calculated such as number of objects per image, object areas (avg, std, max, min), 
#   boxplots of classes areas, bounding boxes areas (avg, std, max, min) and ellipses areas (avg, std, max, min)
# The results will be shown and saved in output_path
analyze: false

####################################
# IMAGE EMBEDDING AND CLUSTERING
####################################

# cluster_images: This parameter allows to study the diversity and similarity of the images using different embedding models and frameworks.
# The images will be embedded using various machine learning models and the resulting embeddings will be analyzed for clustering.
cluster_images: true

# embedding_model: Defines the model and framework used to transform images into numerical feature vectors (embeddings).  
# These embeddings serve as compact representations of images, allowing efficient clustering, similarity search, and other machine-learning tasks.  
# The choice of framework and model impacts the quality, interpretability, and computational cost of embedding extraction. 
embedding_model:

  # framework: Each framework has different pre-trained models to transform images into embeddings that capture their features.
  # These embeddings can then be used for clustering, similarity analysis, or dataset reduction.
  # Options include:
  # - "huggingface": Use Hugging Face models, such as CLIP for general image representation.
  # - "pytorch": Use PyTorch models, including CNN-based models like ResNet for image feature extraction.
  # - "tensorflow": Use TensorFlow models, for image embedding tasks.
  # - "opencv": Use OpenCV Local Binary Patterns (LBP) for texture feature extraction. It is particularly useful for analyzing 
  #   texture patterns and can be applied to image clustering.
  framework: opencv

  # name: Specifies the specific model to be used within the chosen framework.  
  # Models could include "openai/clip-vit-base-patch32" for huggingface, 
  # "resnet50" or "efficientnet_b0" using pytorch,
  # "mobilenet_v2" or "inception_v3" with tensorflow. 
  #  If using "opencv", this field is not be necessary, as LBP is a predefined feature extraction method.
  name: 
  
  # resize_height, resize_width: Resize height and width for images in the dataset to obtain the embeddings with Tensorflow models and OpenCV LBP technique. 
  # Lower values might fasten the proccess, but may also result in loss of fine details in the images.
  # Increasing the value may improve feature extraction quality but will also increase computation time.
  # Default values are 224x224.
  resize_height: 224
  resize_width: 224

  # lbp_radius: Defines the radius (in pixels) of the circular neighborhood around each pixel for calculating Local Binary Patterns (LBP). 
  # A larger radius captures more global texture information, while a smaller radius focuses on finer, local texture details. 
  # Default value to 8.
  lbp_radius: 16

  # lbp_num_points: Specifies the number of sampling points around the central pixel for calculating the LBP. 
  # Higher values result in a more detailed and finer texture representation but also increase computational complexity. 
  # Typical values are 8, 16, and 24. Defaults to 24.
  lbp_num_points: 48

  # lbp_method: Specifies the method used for LBP calculation. 
  # "uniform" method assigns the LBP values to a limited number of patterns, making it more robust and less sensitive to noise, 
  # particularly useful for texture classification. Other methods are "default", "ror", "nri_uniform" or "var".
  lbp_method: uniform

# clustering_models: Defines the clustering model(s) to use with the beforehand calculated embeddings. 
# The possible models are KMeans, AgglomerativeClustering, DBSCAN and OPTICS. 
# You can either specify fixed values for the model's hyperparameters or define a range of values to perform a grid search 
# for finding the best combination of hyperparameters.
clustering_models:

  agglomerative:
    # n_clusters: Defines the number of clusters to form in AgglomerativeClustering.
    # Can be set to a fixed value or a range for grid search.
    n_clusters_range: 
      min: 2
      max: 5
      step: 1

    # linkage: Method used to calculate the linkage between clusters ("ward", "complete", "average", "single").
    # Can be fixed or a range of values can be specified for grid search.
    linkages: [ward]
  
  dbscan:
    # eps_range: Range for the epsilon parameter (eps) in DBSCAN. Defines the maximum distance between two samples 
    # to be considered as neighbors.
    # The range includes a minimum value, maximum value, and step size to create a grid of possible eps values for tuning.
    # It can be a fixed value with establishing "eps" as the hyperparameter.
    eps_range:
      min: 0.1
      max: 2
      step: 0.1

    # min_samples_range: Range for the minimum number of samples required to form a core point in DBSCAN.
    # The range includes a minimum value, maximum value, and step size to perform grid search.
    # It can be a fixed value with establishing "min_samples" as the hyperparameter.
    min_samples_range:
      min: 5
      max: 15
      step: 1
  
  optics:
    # min_samples: Minimum number of samples required to form a cluster in OPTICS. 
    # The min_samples parameter affects how sensitive the algorithm is to noise and the formation of clusters.
    # This can be set as a fixed value.
    min_samples: 5

# clustering_metric: Metric used for evaluating the quality of the clustering. It can be "silhouette" for the Silhouette Score,
# "calisnki" for the Calinski-Harabasz index or "davies" for the Davies-Bouldin index
clustering_metric: calinski

# plot: Flag to enable or disable plotting of the results. If true, plots for the clustering analysis, model performance, etc., will be generated and saved in output_path.
plot: true

# visualization_technique: Defines the dimensionality reduction technique to be used for visualizing the data in lower dimensions (2D) if plot is True. 
# It can be "tsne" (t-Distributed Stochastic Neighbor Embedding) or 
# "pca" (Principal Component Analysis). 
# Defaults to pca.
visualization_technique: tsne

####################################
# DATASET REDUCTION
####################################

# reduction: Flag that indicates whether to reduce the dataset or not. The reduction is done based on clustering results by selecting the most significant images 
# depending on the 'reduction_type' parameter.
reduction: true

# reduction_percentage: Percentage of the original dataset to retain after applying reduction.
reduction_percentage: 0.7

# reduction_type: Strategy to apply when selecting images from each cluster.
# "representative" keeps the most representative images (closest to the center of each cluster) while "diverse" selects those images that are farthest from the center.
# You can also use "random" to select randomly a number of images in each cluster.
# Defaults to representative.
reduction_type: representative

# diverse_percentage: Used when the reduction_type is "representative" or "random". It indicates the percentage of the diverse images to retain from each cluster.
# Defaults to 0.
diverse_percentage: 0.0

# include_outliers: Some clustering models such as DBSCAN or OPTICS, also label outlier values. This flag indicates whether to include 
# outliers (label -1) in the selection. Defaults to false.
include_outliers: false

# reduction_model: Clustering model to apply in order to select the images from each cluster. It can be KMeans, AgglomerativeClustering, DBSCAN or OPTICS. Only one model must 
# be specified.
# It can also use the best model with the best hyperparameters depending on "clustering_metric" from the clustering_models list
# using "best_model" if the cluster_images flag is true.
reduction_model:
  agglomerative:
    # n_clusters: Defines the number of clusters to form in AgglomerativeClustering.
    # Can be set to a fixed value or a range for grid search.
    n_clusters: 5

    # linkage: Method used to calculate the linkage between clusters ("ward", "complete", "average", "single").
    # Can be fixed or a range of values can be specified for grid search.
    linkage: ward

# use_reduced: A flag that determines whether to use the reduced dataset for training models. 
# To enable the use of the reduced dataset, the 'reduction' flag must be set to true.
# Additionally, the original dataset images should have corresponding annotations stored 
# in a "labels" subdirectory within the specified output_path, as indicated in the configuration file.
use_reduced: true

####################################
# TRANSFORMATION CONFIGURATION
####################################

# depth_model: Depth estimation model to use from HuggingFace if needed for converting YOLO and COCO/JSON format labels to multilabel. 
# The model is used to analyze the depth of each object in an image, helping to determine the relative positioning. 
# In cases where multiple objects overlap, the depth model allows us to distinguish which object is closer to the camera 
# and which is further away. This depth information helps to improve the accuracy of the final label by providing a more precise 
# understanding of object relationships, enhancing the quality of the segmentation or detection output.
# Defaults to Intel/dpt-swinv2-tiny-256 model from HuggingFace. If the labels are already in multilabel format or the transformation
# is not needed, this parameter will be ignored.
depth_model: Intel/dpt-swinv2-tiny-256

# threshold: Pixel threshold for binary segmentation labels. If a pixel is greater or equal to the threshold, then it will be marked as 1
# when transforming the original label to a multilabel mask. The rest of the pixels will be marked as 0.
# Defaults to 255. If the labels are not for binary segmentation, this parameter will be ignored.
threshold: 255

# # color_dict: A dictionary that maps specific RGB colors to class labels in the segmentation masks. 
# This is useful for transforming the color-coded segmentation masks into a format compatible with the model, where each 
# color represents a unique object class. The dictionary keys are the RGB values of the color, and the values are the 
# corresponding class labels for segmentation. 
# If the masks are color-coded but no dictionary is provided, one will be calculated automatically by assigning a class to each color,
# starting from 0, in the order of their appearance. If the dictionary is not needed, this parameter will be ignored.
color_dict:
  "[170, 170, 170]": 0     #Trail
  "[0, 255, 0]": 1         #Grass
  "[102, 102, 51]": 2      #Vegetation
  "[0, 0, 0]": 3           #Obstacle
  "[0, 120, 255]": 4       #Sky
  "[0, 60, 0]": 5          #Void

